import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from tensorflow.keras.models import load_model
from data_preprocessing import load_and_preprocess_data
import matplotlib.pyplot as plt
import seaborn as sns

def evaluate_model(model, X_test, y_test):
    """
    Evaluate the model using accuracy, precision, recall, F1-score, and confusion matrix.
    Args:
        model: Trained model to evaluate
        X_test: Test features
        y_test: Test labels
    Returns:
        metrics: Dictionary containing all evaluation metrics
    """
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)

    accuracy = accuracy_score(y_test, y_pred_classes)
    precision = precision_score(y_test, y_pred_classes, average='binary')
    recall = recall_score(y_test, y_pred_classes, average='binary')
    f1 = f1_score(y_test, y_pred_classes, average='binary')
    cm = confusion_matrix(y_test, y_pred_classes)

    metrics = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'confusion_matrix': cm
    }

    return metrics

def plot_confusion_matrix(cm, labels):
    """
    Plot the confusion matrix.
    Args:
        cm: Confusion matrix
        labels: List of class labels
    """
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.show()

def test_models():
    # Load test data (assuming the preprocessing is done similarly as during training)
    X_test, y_test = load_and_preprocess_data('datasets/IoT-23.csv')  # Replace with your dataset

    # Load models (after training)
    cnn_model = load_model('models/cnn_model.h5')
    lstm_model = load_model('models/lstm_model.h5')
    hybrid_model = load_model('models/hybrid_model.h5')

    # Evaluate models
    cnn_metrics = evaluate_model(cnn_model, X_test, y_test)
    lstm_metrics = evaluate_model(lstm_model, X_test, y_test)
    hybrid_metrics = evaluate_model(hybrid_model, X_test, y_test)

    # Print metrics
    print("CNN Model Evaluation:")
    print(cnn_metrics)
    
    print("\nLSTM Model Evaluation:")
    print(lstm_metrics)
    
    print("\nHybrid Model Evaluation:")
    print(hybrid_metrics)

    # Plot confusion matrix for each model
    labels = ['Normal', 'Attack']
    print("\nConfusion Matrix for CNN:")
    plot_confusion_matrix(cnn_metrics['confusion_matrix'], labels)

    print("\nConfusion Matrix for LSTM:")
    plot_confusion_matrix(lstm_metrics['confusion_matrix'], labels)

    print("\nConfusion Matrix for Hybrid Model:")
    plot_confusion_matrix(hybrid_metrics['confusion_matrix'], labels)

# Run the testing function
if __name__ == "__main__":
    test_models()
